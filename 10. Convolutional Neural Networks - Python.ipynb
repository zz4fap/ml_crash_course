{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Convolutional Neural Networks\n======\n\nConvolutional neural networks (CNNs) are a class of deep neural networks, most commonly used in computer vision applications.\n\nConvolutional refers the network pre-processing data for you - traditionally this pre-processing was performed by data scientists. The neural network can learn how to do pre-processing *itself* by applying filters for things such as edge detection."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 1\n-----\n\nIn this exercise we will train a CNN to recognise handwritten digits, using the MNIST digit dataset.\n\nThis is a very common exercise and data set to learn from.\n\nLet's start by loading our dataset and setting up our train, validation, and test sets.\n\n#### Run the code below to import our required libraries and set up the graphing features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run this!\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport tensorflow as tf\nimport numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\nprint('keras using %s backend'%keras.backend.backend())\nimport matplotlib.pyplot as graph\n%matplotlib inline\ngraph.rcParams['figure.figsize'] = (15,5)\ngraph.rcParams[\"font.family\"] = 'DejaVu Sans'\ngraph.rcParams[\"font.size\"] = '12'\ngraph.rcParams['image.cmap'] = 'rainbow'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### In the cell below replace:\n#### 1. `<addTrainX>` with `train_X`\n#### 2. `<addTrainY>` with `train_Y`\n#### 3. `<addValidX>` with `valid_X`\n#### 4. `<addValidY>` with `valid_Y`\n#### 5. `<addTextX>` with `test_X`\n#### 6. `<addTextY>` with `test_Y`\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Here we import the dataset, and split it into the training, validation, and test sets.\nfrom keras.datasets import mnist\n\n# This is our training data, with 6400 samples.\n###\n# REPLACE <addTrainX> WITH train_X AND <addTrainY> WITH train_Y\n###\n<addTrainX> = mnist.load_data()[0][0][:6400].astype('float32')\n<addTrainY> = mnist.load_data()[0][1][:6400]\n###\n\n# This is our validation data, with 1600 samples.\n###\n# REPLACE <addValidX> WITH valid_X AND <addValidY> WITH valid_Y\n###\n<addValidX> = mnist.load_data()[1][0][:1600].astype('float32')\n<addValidY> = mnist.load_data()[1][1][:1600]\n###\n\n# This is our test data, with 2000 samples.\n###\n# REPLACE <addTextX> WITH test_X AND <addTextY> WITH test_Y\n###\n<addTextX> = mnist.load_data()[1][0][-2000:].astype('float32')\n<addTextY> = mnist.load_data()[1][1][-2000:]\n###\n\nprint('train_X:', train_X.shape, end = '')\nprint(', train_Y:', train_Y.shape)\nprint('valid_X:', valid_X.shape, end = '')\nprint(', valid_Y:', valid_Y.shape)\nprint('test_X:', test_X.shape, end = '')\nprint(', test_Y:', test_Y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So we have 6400 training samples, 1600 validation samples, and 2000 test samples.\n\nEach sample is an greyscale image - 28 pixels wide and 28 pixels high. Each pixel is really a number from 0 to 255 - 0 being fully black, 255 being fully white. When we graph the 28x28 numbers, we can see the image.\n\nLet's have a look at one of our samples.\n\n#### Replace `<addSample>` with `train_X[0]` (you can change 0 to any number between 0 and 6400 if you like)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE THE <addSample> BELOW WITH train_X[0] OR ANOTHER SAMPLE e.g. train_X[1] or train_X[2]\n###\ngraph.imshow(<addSample>, cmap = 'gray', interpolation = 'nearest')\n###\n\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 2\n---\n\nThe neural network will use the 28x28 values of each image to predict what each image represents.\n\nAs each value is between 0 and 255, we'll scale the values down by dividing by 255 (this makes it faster for the Neural Network to train).\n\nWe need to reshape our data to get it working well with our neural network. \n\n### In the cell below replace:\n#### 1. `<addRehape>` with `reshape`\n#### 2. `<completeCalculation>` with `/255`\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# First off, let's reshape our X sets so that they fit the convolutional layers.\n\n# This gets the image dimensions - 28\ndim = train_X[0].shape[0]\n\n###\n# REPLACE THE <addRehape> BELOW WITH reshape\n###\ntrain_X = train_X.<addRehape>(train_X.shape[0], dim, dim, 1)\nvalid_X = valid_X.<addRehape>(valid_X.shape[0], dim, dim, 1)\ntest_X = test_X.<addRehape>(test_X.shape[0], dim, dim, 1)\n###\n\n# Next up - feature scaling.\n# We scale the values so they are between 0 and 1, instead of 0 and 255.\n\n###\n# REPLACE THE <completeCalculation> BELOW WITH /255\n###\ntrain_X = train_X <completeCalculation>\nvalid_X = valid_X <completeCalculation>\ntest_X = test_X <completeCalculation>\n###\n\n\n# Now we print the label for the first example\nprint(train_Y[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Expected output:  \n`5`\n\nThe label is a number - the number we see when we view the image.\n\nWe need represent this number as a one-hot vector, so the neural network knows it is a category.\n\nKeras can convert these labels into one-hot vectors easily with the function - `to_categorical`\n\n#### Replace `<addCategorical>` with `to_categorical`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE THE <addCategorical> BELOW WITH to_categorical\n###\ntrain_Y = keras.utils.<addCategorical>(train_Y, 10)\nvalid_Y = keras.utils.<addCategorical>(valid_Y, 10)\ntest_Y = keras.utils.<addCategorical>(test_Y, 10)\n###\n\n# 10 being the number of categories (numbers 0 to 9)\n\nprint(train_Y[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Expected output:  \n`[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]`\n\nStep 3\n-----\n\nAll ready! Time to build another neural network.\n\n#### Replace `<addSequential>` with `Sequential()` and run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Sets a randomisation seed for replicatability.\nnp.random.seed(6)\n\n###\n# REPLACE THE <addSequential> BELOW WITH Sequential() (don't forget the () )\n###\nmodel = <addSequential>\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The __Convolutional__ in Convolutional Neural Networks refers the pre-processing the network can do itself.\n\n#### Replace `<addConv2d>` with `Conv2D`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE THE <addConv2D> BELOW WITH Conv2D\n###\nmodel.add(<addConv2D>(28, kernel_size = (3, 3), activation = 'relu', input_shape = (dim, dim, 1)))\nmodel.add(<addConv2D>(56, (3, 3), activation = 'relu'))\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next up we'll:\n* Add pooling layers.\n* Apply dropout.\n* Flatten the data to a vector (the output of step 2 is a vector).\n\n### In the cell below replace:\n#### 1. `<addMaxPooling2D>` with `MaxPooling2D`\n#### 2. `<addDropout>` with `Dropout`\n#### 3. `<addFlatten>` with `Flatten()`\n\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Pooling layers help speed up training time and make features it detects more robust.\n# They act by downsampling the data - reducing the data size and complexity.\n\n###\n# REPLACE THE <addMaxPooling2D> BELOW WITH MaxPooling2D\n###\nmodel.add(<addMaxPooling2D>(pool_size = (2, 2)))\n###\n\n# Dropout is a technique to help prevent overfitting\n# It makes nodes 'dropout' - turning them off randomly.\n\n###\n# REPLACE THE <addDropout> BELOW WITH Dropout\n###\nmodel.add(<addDropout>(0.125))\n###\n\n\n###\n# REPLACE THE <addFlatten> BELOW WITH Flatten()\n###\nmodel.add(<addFlatten>)\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Replace `<updateHere>` with 10 and run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Dense layers perform classification - we have extracted the features with the convolutional pre-processing\nmodel.add(Dense(128, activation='relu'))\n\n# More dropout!\nmodel.add(Dropout(0.25))\n\n# Next is our output layer\n# Softmax outputs the probability for each category\n###\n# REPLACE <updateHere> BELOW WITH 10, THE NUMBER OF CLASSES (DIGITS 0 TO 9)\n###\nmodel.add(Dense(<updateHere>, activation=tf.nn.softmax))\n###\n\n# And finally, we compile.\nmodel.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 4\n-----\n\nLet's train it!\n\n### In the cell below replace:\n#### 1. `<addTrainX>` with `train_X `\n#### 2. `<addTrainY>` with `train_Y`\n#### 3. `<addValidX>` with `valid_X`\n#### 4. `<addValidY>` with `valid_Y`\n#### 5. `<addEvaluate>` with `evaluate`\n\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE THE <addTrainX> WITH train_X, <addTrainY> WITH train_Y, <addValidX> WITH valid_X, AND <addValidY> WITH valid_Y\n###\ntraining_stats = model.fit(<addTrainX>, <addTrainY>, batch_size = 128, epochs = 12, verbose = 1, validation_data = (<addValidX>, <addValidY>))\n###\n\n###\n# REPLACE THE <addEvaluate> BELOW WITH evaluate\n###\nevaluation = model.<addEvaluate>(test_X, test_Y, verbose=0)\n###\n\nprint('Test Set Evaluation: loss = %0.6f, accuracy = %0.2f' %(evaluation[0], 100 * evaluation[1]))\n\n# We can plot our training statistics to see how it developed over time\naccuracy, = graph.plot(training_stats.history['acc'], label = 'Accuracy')\ntraining_loss, = graph.plot(training_stats.history['loss'], label = 'Training Loss')\ngraph.legend(handles = [accuracy, training_loss])\nloss = np.array(training_stats.history['loss'])\nxp = np.linspace(0,loss.shape[0],10 * loss.shape[0])\ngraph.plot(xp, np.full(xp.shape, 1), c = 'k', linestyle = ':', alpha = 0.5)\ngraph.plot(xp, np.full(xp.shape, 0), c = 'k', linestyle = ':', alpha = 0.5)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 5\n\nLet's test it on a new sample that it hasn't seen, and see how it classifies it!\n\n#### Replace `<addNumber>` with any number between 0 and 1999, then run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE THE <addNumber> WITH ANY NUMBER BETWEEN 0 AND 1999\n###\nsample = test_X[<addNumber>].reshape(dim, dim)\n###\n\ngraph.imshow(sample, cmap = 'gray', interpolation = 'nearest')\ngraph.show()\n\nprediction = model.predict(sample.reshape(1, dim, dim, 1))\nprint('prediction: %i' %(np.argmax(prediction)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How is the prediction? Does it look right?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Conclusion\n------\n\nCongratulations! We've built a convolutional neural network that is able to recognise handwritten digits with very high accuracy.\n\nCNN's are very complex - you're not expected to understand everything (or most things) we covered here. They take a lot of time and practise to properly understand each aspect of them.\n\nHere we used:  \n* __Feature scaling__ - reducing the range of the values. This helps improve training time.\n* __Convolutional layers__ - network layers that pre-process the data for us. These apply filters to extract features for the neural network to analyze.\n* __Pooling layers__ - part of the Convolutional layers. They apply filters downsample the data - extracting features.\n* __Dropout__ - a regularization technique to help prevent overfitting.\n* __Dense layers__ - neural network layers which perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers.\n* __Softmax__ - an activation function which outputs the probability for each category."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}