{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Clustering\n======\n\nWhen a data set doesnâ€™t have labels we can use unsupervised learning to find some kind of structure in the data - allowing us to discover patterns or groupings.\n\nCluster analysis is a method of finding groupings, known as clusters, in datasets. As the data sets are unlabelled, cluster analysis tries to group similar examples using the examples features.\n\nK-means clustering lives true to its name - it separates examples into k number of clusters (so if k is 5, it will divide the examples into 5 clusters) and it partitions the examples by the average (mean) of the clusters."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 1\n-----\n\nIn this exercise we will look at using k-means clustering to categorise a few different datasets.\n\nLet's start by first creating three clusters.\n\n#### Run the code below to set up the graphing features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This sets up the graphs\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as graph\n%matplotlib inline\ngraph.rcParams['figure.figsize'] = (15,5)\ngraph.rcParams[\"font.family\"] = 'DejaVu Sans'\ngraph.rcParams[\"font.size\"] = '12'\ngraph.rcParams['image.cmap'] = 'rainbow'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### In the cell below replace:\n#### 1. `<addClusterData>` with `cluster_data`\n#### 2. `<addOutput>` with `output`\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Let's make some data!\nimport numpy as np\nfrom sklearn import datasets\n\n###\n# REPLACE <addClusterData> WITH cluster_data AND <addOutput> WITH output\n###\n<addClusterData>, <addOutput> = datasets.make_classification(n_samples = 500, n_features = 2, n_informative = 2, n_redundant = 0, n_repeated = 0,\n                                                    n_classes = 3, n_clusters_per_class = 1, class_sep = 1.25, random_state = 6)\n###\n\n# Let's visualise it\ngraph.scatter(cluster_data.T[0], cluster_data.T[1])\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's see how k-means performs on a dataset like this!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### In the cell below replace:\n#### 1. `<addKMeans>` with `KMeans`\n#### 2. `<addFit>` with `fit`\n#### 3. `<addClusterCenters>` with `k_means.cluster_centers_`\n#### 4. `<addLabels>` with `k_means.labels_`\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans\n\n###\n# REPLACE <addKMeans> WITH KMeans\n###\nk_means = <addKMeans>(n_clusters=3)\n###\n\n###\n# REPLACE <addFit> WITH fit\n###\nk_means.<addFit>(cluster_data)\n###\n\n# Let's visualise it\n###\n# REPLACE <addClusterCenters> BELOW WITH k_means.cluster_centers_\n###\nfor mean in <addClusterCenters>:\n    graph.plot(mean[0], mean[1], 'ko', marker = '+', markersize = 20)\n###\n\n###\n# REPLACE <addLabels> BELOW WITH k_means.labels_\n###\ngraph.scatter(cluster_data.T[0], cluster_data.T[1], c = <addLabels>)\n###\n\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It performs rather well, by the looks of it! But we already knew that it had three clusters, sometimes it might not be so clear. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 2\n\nLet's generate another dataset in which it may be a little less obvious how many classes it contains.\n\n#### Replace `<addMakeClassification>` with `datasets.make_classification` and run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE <addMakeClassification> BELOW WITH datasets.make_classification\n###\ncluster_data, output = <addMakeClassification>(n_samples = 500, n_features = 2, n_informative = 2, n_redundant = 0, n_repeated = 0, \n                                            n_classes = 4, n_clusters_per_class = 1, class_sep = 1.25, random_state = 6)\n###\n\ngraph.scatter(cluster_data.T[0], cluster_data.T[1])\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In instances where we do not know how many classes to expect, it is handy to run k-means multiple times and compare how the data looks when divided up into a differing number of classes. Let's try that now.\n\n#### Replace `<addNHere>` with `n` and run the code"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE <addNHere> BELOW WITH n\n###\nfor <addNHere> in range(2,6):\n    k_means = KMeans(n_clusters = <addNHere>).fit(cluster_data)\n###\n\n    for mean in k_means.cluster_centers_:\n        graph.plot(mean[0], mean[1], 'ko', marker = '+', markersize = 20)\n    graph.scatter(cluster_data.T[0], cluster_data.T[1], c = k_means.labels_)\n    graph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Which one do you think best splits the data?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 3\n========\n\nK-means clustering performs well enough on clustered data like that, but let's try it out on a dataset that is not so linear.\n\n#### Replace `<addMakeCircles>` with `make_circles` and run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE <addMakeCircles> BELOW WITH make_circles\n###\nring_data, target = datasets.<addMakeCircles>(n_samples = 500, factor = .5, noise = 0.05, random_state = 6)\n###\n\ngraph.scatter(ring_data.T[0], ring_data.T[1], c = target)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can clearly distinguish two \"clusters\", that is, the two rings of datapoints.\n\nLet's see how k-means handles a dataset like this.\n\n#### Replace `<addRingData>` with `ring_data` and run the code"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE <addRingData> BELOW WITH ring_data\n###\nk_means = KMeans(n_clusters = 2).fit(<addRingData>)\n###\n\nfor mean in k_means.cluster_centers_:\n    graph.plot(mean[0], mean[1], 'ko', marker = '+', markersize = 20)\ngraph.scatter(ring_data.T[0], ring_data.T[1], c = k_means.labels_)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "K-means clearly has difficulty solving this.\n\nAs we are using it, there is no way for k-means to place two means to label this data set correctly."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 4\n------\n\nBut, we can try another way. We can use another feature - distance away from the centre.\n\nLet's see if k-means is able to classify the two data clusters with this new feature.\n\n#### Replace `<addSqrt>` with `np.sqrt` and run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "distance_from_center = []\nfor sample in ring_data:\n###\n# REPLACE <addSqrt> BELOW WITH np.sqrt\n###\n    z = 4 * <addSqrt>(sample[0]**2 + sample[1]**2)\n###\n    distance_from_center.append(z)\n# Make it a three-dimensional dataset\nring_data = np.concatenate((ring_data, np.array(distance_from_center).reshape(-1, 1)), axis = 1)\n\ngraph.scatter(ring_data.T[0], ring_data.T[1], c = ring_data.T[2])\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Looks like it will work, so let's plot all three features.\n\n### In the cell below replace:\n#### 1. `<addProjection>` with `projection='3d'`\n#### 2. `<addRingDataT>` with `ring_data.T[2]`\n#### and then __run the code__. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from mpl_toolkits.mplot3d import Axes3D\n\nfig = graph.figure()\n###\n# REPLACE <addProjection> BELOW WITH projection='3d'\n###\nax = fig.add_subplot(111, <addProjection>)\n###\n\n###\n# REPLACE <addRingDataT> BELOW WITH ring_data.T[2]\n###\nax.scatter(ring_data.T[0], ring_data.T[1], <addRingDataT>, c = target)\n###\n\nax.view_init(30, 45)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's see how k-means deals with the data now that it has 3 features!\n\n### In the cell below replace:\n#### 1. `<addRingData>` with `ring_data`\n#### 2. `<addLabels>` with `k_means.labels_`\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE <addRingData> BELOW WITH ring_data\n###\nk_means = KMeans(n_clusters = 2, random_state = 0).fit(<addRingData>)\n###\n\nfig = graph.figure()\nax = fig.add_subplot(111, projection='3d')\nfor mean in k_means.cluster_centers_:\n    ax.scatter(mean[0], mean[1], mean[2], c='black', marker='+', s=50) # plot the cluster centres\n    \n###\n# REPLACE <addLabels> BELOW WITH k_means.labels_\n###\nax.scatter(ring_data.T[0], ring_data.T[1], ring_data.T[2], c = <addLabels>)\n###\n\n# We can plot a hyperplane that separates the two rings\nhp_X, hp_Y = np.array(np.meshgrid(np.linspace(-1, 1, 11), np.linspace(-1, 1, 11)))\nhp_Z = np.full(hp_X.shape, np.abs(k_means.cluster_centers_[0][2] - k_means.cluster_centers_[1][2] / 2))\nax.plot_wireframe(hp_X, hp_Y, hp_Z, rstride = 1, cstride = 1, \n                  color = 'k', linewidth = 1, linestyle = 'solid', alpha = 0.5)\n\nax.view_init(20, 45)\nax.set_zlabel('new axis')\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can see the `+` that indicates the center of the clusters. Looks good!\n\nStep 5\n------\n\nSome data we cannot manipulate like that. Let's have a look at a different type of data distribution.\n\n#### Replace `<addMakeMoons>` with `datasets.make_moons` and run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE <addMakeMoons> BELOW WITH datasets.make_moons\n###\ncrescent_data, output = <addMakeMoons>(n_samples = 500, noise = .05)\n###\n\ngraph.scatter(crescent_data.T[0], crescent_data.T[1], c = target)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try fitting it.\n\n#### Replace `<addCrescentData>` with `crescent_data` and run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Below we run KMeans on crescent_data using n_clusters = 2\n###\n# REPLACE <addCrescentData> WITH crescent_data\n###\nk_means = KMeans(n_clusters = 2).fit(<addCrescentData>)\n###\n\nfor mean in k_means.cluster_centers_:\n    graph.plot(mean[0], mean[1], 'ko', marker = '+', markersize = 20)\ngraph.scatter(crescent_data.T[0], crescent_data.T[1], c = k_means.labels_)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, a similar issue as with the circle data.\n\nBut k-means is just one method for clustering, other methods don't have quite the same restrictions as k-means.\n\nStep 6\n------\n\nSpectral clustering is a clustering method that aims to cluster data that is in some way connected - but not necessarily distributed.\n\n### In the cell below replace:\n#### 1. `<addSpectralClustering>` with `SpectralClustering`\n#### 2. `<addCrescentData>` with `crescent_data`\n#### 3. `<addLabels>` with `labels_`\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import cluster\n\n###\n# REPLACE <addSpectralClustering> BELOW WITH SpectralClustering\n###\nspectral = cluster.<addSpectralClustering>(n_clusters = 2, eigen_solver = 'arpack', affinity = 'nearest_neighbors')\n###\n\n###\n# REPLACE <addCrescentData> BELOW WITH crescent_data\n###\nlabels_ = spectral.fit_predict(<addCrescentData>)\n###\n\n### \n# REPLACE <addLabels> BELOW WITH labels_\n###\ngraph.scatter(crescent_data.T[0], crescent_data.T[1], c = <addLabels>)\n###\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### In the cell below replace:\n#### 1. `<addSpectralClustering>` with `SpectralClustering`\n#### 2. `<addRingData>` with `ring_data`\n#### 3. `<addLabels>` with `labels_`\n#### and then __run the code__."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Let's use spectral clustering on the ring_data\n\n###\n# REPLACE <addSpectralClustering> BELOW WITH SpectralClustering\n###\nspectral = cluster.<addSpectralClustering>(n_clusters = 2, eigen_solver = 'arpack', affinity = 'nearest_neighbors')\n###\n\n###\n# REPLACE <addRingData> BELOW WITH ring_data\n###\nlabels_ = spectral.fit_predict(<addRingData>)\n###\n\n###\n# REPLACE <addLabels> BELOW WITH labels_\n###\ngraph.scatter(ring_data.T[0], ring_data.T[1], c = <addLabels>)\n###\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Does it classify the data in the correct clusters?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Conclusion\n\nWe have learnt two important clustering methods, k-means and spectral clustering, and used them on a variety of datasets where one might be more appropriate to use than the other."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}